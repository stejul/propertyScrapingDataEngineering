
# Project Title

A example project where I test out different data engineering tools, like Apache Druid, PySPark and Dagster.

With scrapy im extracting some key information on various apartments, load them up into a S3 storage and run it in an DWH.


## Tech Stack

- Scrapy
- Dagster
- Apache Druid
- Docker
- Apache Superset
- Jupyter Notebook
- Min.io (https://min.io/)
- PySpark

## Environment Variables

To run this project, you will need to add the following environment variables to your .env file

`MINIO_USER`

`MINIO_PASSWORD`


## Installation

Install my-project with pip or poetry

```bash
  pip install -r requirements.txt
```
Or
```bash
  poetry install
```

## Run Locally

Clone the project

```bash
  git clone https://github.com/stejul/dataEngineeringExample
```

Install dependencies
```bash
  poetry install
```
or `pip install -r requirements.txt`

Start the server

```bash

```


# WIP SECTION
## Running Tests

To run tests, run the following command

```bash
  npm run test
```


## Deployment

To deploy this project run

```bash
  npm run deploy
```


## Lessons Learned

What did you learn while building this project? What challenges did you face and how did you overcome them?


## License

[MIT](https://choosealicense.com/licenses/mit/)


## Authors

- [@stejul](https://www.github.com/stejul)
